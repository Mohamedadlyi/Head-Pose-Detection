{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsCS8ckGAua6"
   },
   "source": [
    "### Installing MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AfVHCj5VAkky"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install mediapipe\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNdGPfI2A0vD"
   },
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JMD9H2caAyos"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from math import cos, sin\n",
    "import moviepy.editor as mpimg\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import Image \n",
    "import subprocess\n",
    "import pickle\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oZVe65YyFJdF"
   },
   "outputs": [],
   "source": [
    "def draw_axis(img, pitch,yaw,roll, tdx=None, tdy=None, size = 100):\n",
    "\n",
    "    yaw = -yaw\n",
    "    if tdx != None and tdy != None:\n",
    "        tdx = tdx\n",
    "        tdy = tdy\n",
    "    else:\n",
    "        height, width = img.shape[:2]\n",
    "        tdx = width / 2\n",
    "        tdy = height / 2\n",
    "\n",
    "    # X-Axis pointing to right. drawn in red\n",
    "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
    "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
    "\n",
    "    # Y-Axis | drawn in green\n",
    "    #        v\n",
    "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
    "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
    "\n",
    "    # Z-Axis (out of the screen) drawn in blue\n",
    "    x3 = size * (sin(yaw)) + tdx\n",
    "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),3)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(face, height = 450, width = 450):\n",
    "\n",
    "    x_val = [lm.x*width  for lm in face]\n",
    "    y_val = [lm.y*height  for lm in face]\n",
    "\n",
    "    m_val = x_val[:468] +y_val[:468]\n",
    "    \n",
    "    m_val = np.array(m_val)-np.mean(m_val)\n",
    "\n",
    "    m_val = m_val / m_val.max()\n",
    "    \n",
    "    return m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_with_axis(image, detector):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image to extract landmark points\n",
    "    image_mp = mp.Image(image_format=mp.ImageFormat.SRGB,data = image_rgb)\n",
    "\n",
    "    results = detector.detect(image_mp)\n",
    "    \n",
    "    # Check if landmarks were detected\n",
    "    if results.face_landmarks:\n",
    "        image_with_axes2 = image\n",
    "        for face in results.face_landmarks:\n",
    "            angles = model.predict(preprocess(face).reshape(1,-1))\n",
    "            pitch, yaw, roll = angles[0,0], angles[0,1], angles[0,2]\n",
    "            center = face[4]\n",
    "            image_with_axes2 = draw_axis(image_with_axes2, pitch, yaw, roll, tdx = center.x * image.shape[1], tdy = center.y * image.shape[0])\n",
    "        return image_with_axes2\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path, detector):\n",
    "    t1 = time.time()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file.\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    \n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (2*frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    max_frame_count = np.inf\n",
    "    while frame_count < max_frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        processed_frame = get_img_with_axis(frame.copy(), detector)\n",
    "        concatenated_frame = np.concatenate((frame, processed_frame), axis=1)\n",
    "        out.write(concatenated_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "\n",
    "    t2 = time.time()\n",
    "    processing_time = t2 - t1\n",
    "    print(f\"Processing time: {processing_time}, for video of length {frame_count} frames.\")\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    video_clip = mpimg.VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(output_audio_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that the model is downloaded correctly\n",
    "\n",
    "filename = 'SVR_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then download the off-the-shelf model bundle(s). Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#models) for more information about these model bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure The Face Detector Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: DRI2: failed to authenticate\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709515751.236978   57481 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1709515751.247834   57585 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: llvmpipe (LLVM 15.0.7, 256 bits)\n",
      "W0000 00:00:1709515751.248512   57481 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# make sure that 'face_landmarker_v2_with_blendshapes.task' is loaded\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=3,\n",
    "                                      min_face_detection_confidence = 0.4,\n",
    "                                      running_mode=vision.RunningMode.IMAGE)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a video with the Head pose marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 89.3348650932312, for video of length 1212 frames.\n",
      "MoviePy - Writing audio in ./out/test6.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "counter = '6'\n",
    "video_path = f'./content/test{counter}.webm'\n",
    "output_path = f'./out/test_marked{counter}.avi'\n",
    "output_audio_path = f'./out/test{counter}.mp3'\n",
    "final_path = f'./out/test{counter}.mp4'\n",
    "\n",
    "process_video(video_path, output_path,detector)\n",
    "# Add audio to the output video\n",
    "extract_audio(video_path, output_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, avi, from './out/test_marked6.avi':\n",
      "  Metadata:\n",
      "    software        : Lavf59.27.100\n",
      "  Duration: 00:00:41.79, start: 0.000000, bitrate: 35976 kb/s\n",
      "  Stream #0:0: Video: mjpeg (Baseline) (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 2160x1920, 35989 kb/s, 29 fps, 29 tbr, 29 tbn, 29 tbc\n",
      "Input #1, mp3, from './out/test6.mp3':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:40.49, start: 0.025057, bitrate: 128 kb/s\n",
      "  Stream #1:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "  Stream #1:0 -> #0:1 (mp3 (mp3float) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to './out/test6.mp4':\n",
      "  Metadata:\n",
      "    software        : Lavf59.27.100\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: mjpeg (Baseline) (mp4v / 0x7634706D), yuvj420p(pc, bt470bg/unknown/unknown), 2160x1920, q=2-31, 35989 kb/s, 29 fps, 29 tbr, 14848 tbn, 29 tbc\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 aac\n",
      "frame= 1212 fps=0.0 q=-1.0 Lsize=  184103kB time=00:00:41.75 bitrate=36116.4kbits/s speed=89.5x    \n",
      "video:183456kB audio:611kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.019262%\n",
      "[aac @ 0x55663eaebe00] Qavg: 8813.022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the audio to the output marked video\n",
    "ffmpeg_command = f\"ffmpeg -y -i {output_path} -i {output_audio_path} -c:v copy -c:a aac -strict experimental '{final_path}'\"\n",
    "subprocess.call(ffmpeg_command, shell=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MediaPipe Helper Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
